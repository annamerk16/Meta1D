# Meta1D
Meta 1D 
ğŸ‘¥ Team Members
| Name             | GitHub Handle | Contribution                                                             |
|-------------------|----------------|--------------------------------------------------------------------------|
| Yuling Liu        | @yuuuu1ing     | Data exploration, visualization, overall project coordination            |
| Alisha Varma      | @alishav14     | Data collection, exploratory data analysis (EDA), dataset documentation  |
| Bhavana Chemuturi | @bhavanachem   | Data preprocessing, feature engineering, data validation                 |
| Anna Merkulova    | @annamerk16    | Model selection, hyperparameter tuning, model training and optimization  |
| Iid Maxamuud      | @abdifatah2023 | Model evaluation, performance analysis, results interpretation           |
| Daniela Suqui     | @danielasqui   | Model evaluation, performance analysis, results interpretation           |



ğŸ¯ Project Highlights

- Developed NLP techniques to detect and quantify social biases in text datasets using word embeddings and classification models.
- Identified biased semantic relationships and evaluated their impact on model predictions, supporting Metaâ€™s Responsible AI initiatives.
- Created heatmaps, embedding plots, and bias trend metrics to clearly communicate findings.
- Delivered a fully documented, reproducible codebase aligned with ethical AI standards.

ğŸ‘©ğŸ½â€ğŸ’» Setup and Installation




ğŸ—ï¸ Project Overview

This project was completed as part of the Break Through Tech AI Program in partnership with Meta.

Objective

- Identify and analyze social biases present in NLP datasets, understand how they influence model outputs, and evaluate techniques for interpreting or mitigating bias.
  
Scope

- Detect biased terms and associations
- Use word embeddings to visualize semantic relationships
- Train simple classifiers to observe bias-driven prediction patterns
- Document findings and provide visual tools for interpretability

Real-World Significance

- Bias in training data can perpetuate harmful stereotypes and lead to inequitable AI systems. Because Meta's platforms serve billions globally, mitigating dataset bias is essential for safety, fairness, and compliance with ethical AI standards.

ğŸ“Š Data Exploration


ğŸ§  Model Development


ğŸ“ˆ Results & Key Findings


ğŸš€ Next Steps

- Expand datasets (especially religion-related biases; add non-Reddit sources).
- Test larger transformer models (e.g., RoBERTa-large).
- Apply domain adaptation techniques to improve cross-dataset robustness.
- Conduct deeper error analysis to reduce false positives/negatives.
- Improve user-facing prototype for wider deployment.

ğŸ“ License

ğŸ“„ References

ğŸ™ Acknowledgements

- Megan Ung, Meta Challenge Advisor
- Candace Ross, Meta Challenge Advisor
- Rajshri Jain, AI Studio Coach
- Break Through Tech AI Program
- Peer collaborators and mentors who supported our project
